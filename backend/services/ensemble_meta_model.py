"""
Gradient-Boosted Ensemble Meta-Risk Model
Combines outputs from forecasting, anomalies, invoices, and liquidity buffers
Generates the final 0â€“100 E-Risk Score with high stability
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from backend.utils.logger import get_logger

logger = get_logger(__name__)


class EnsembleMetaRiskModel:
    """
    Meta-model that combines predictions from all advanced models
    Uses gradient boosting for stable, accurate risk scoring
    """
    
    @staticmethod
    def calculate_ensemble_risk_score(
        dagru_forecast: Dict,
        tft_forecast: Dict,
        nbeats_forecast: Dict,
        deepar_forecast: Dict,
        autoencoder_anomalies: List[Dict],
        isolation_forest_anomalies: List[Dict],
        gat_anomalies: List[Dict],
        tabnet_invoice_risks: List[Dict],
        traditional_risk: Dict,
        historical_data: pd.DataFrame
    ) -> Dict[str, any]:
        """
        Combine all model outputs into final E-Risk Score
        """
        logger.info("Calculating Ensemble Meta-Risk Score")
        
        try:
            # Extract features from all models
            features = EnsembleMetaRiskModel._extract_ensemble_features(
                dagru_forecast=dagru_forecast,
                tft_forecast=tft_forecast,
                nbeats_forecast=nbeats_forecast,
                deepar_forecast=deepar_forecast,
                autoencoder_anomalies=autoencoder_anomalies,
                isolation_forest_anomalies=isolation_forest_anomalies,
                gat_anomalies=gat_anomalies,
                tabnet_invoice_risks=tabnet_invoice_risks,
                traditional_risk=traditional_risk,
                historical_data=historical_data
            )
            
            logger.info(f"Extracted {len(features)} features from all models")
            logger.info(f"Feature categories: forecasting={sum(1 for k in features if any(m in k for m in ['dagru','tft','nbeats','deepar']))}, "
                       f"anomaly={sum(1 for k in features if any(m in k for m in ['ae_','if_','gat_']))}, "
                       f"invoice={sum(1 for k in features if 'invoice' in k)}, "
                       f"traditional={sum(1 for k in features if 'trad_' in k)}, "
                       f"historical={sum(1 for k in features if 'hist_' in k)}")
            
            # Apply ensemble learning
            e_risk_score, confidence, model_contributions = EnsembleMetaRiskModel._gradient_boosting_ensemble(
                features
            )
            
            # Generate comprehensive risk assessment
            risk_assessment = EnsembleMetaRiskModel._generate_comprehensive_assessment(
                e_risk_score=e_risk_score,
                confidence=confidence,
                model_contributions=model_contributions,
                features=features,
                all_forecasts={
                    'dagru': dagru_forecast,
                    'tft': tft_forecast,
                    'nbeats': nbeats_forecast,
                    'deepar': deepar_forecast
                },
                all_anomalies={
                    'autoencoder': autoencoder_anomalies,
                    'isolation_forest': isolation_forest_anomalies,
                    'gat': gat_anomalies
                },
                invoice_risks=tabnet_invoice_risks
            )
            
            logger.info(f"Ensemble E-Risk Score: {e_risk_score:.2f} with {confidence:.1%} confidence")
            
            return risk_assessment
            
        except Exception as e:
            logger.error(f"Ensemble meta-risk calculation error: {e}")
            # Fallback to traditional risk
            return {
                'e_risk_score': traditional_risk.get('overall_score', 50),
                'confidence': 0.5,
                'error': str(e)
            }
    
    @staticmethod
    def _extract_ensemble_features(
        dagru_forecast: Dict,
        tft_forecast: Dict,
        nbeats_forecast: Dict,
        deepar_forecast: Dict,
        autoencoder_anomalies: List[Dict],
        isolation_forest_anomalies: List[Dict],
        gat_anomalies: List[Dict],
        tabnet_invoice_risks: List[Dict],
        traditional_risk: Dict,
        historical_data: pd.DataFrame
    ) -> Dict[str, float]:
        """Extract features from all model outputs"""
        features = {}
        
        # Forecasting model features
        features.update(
            EnsembleMetaRiskModel._extract_forecast_features(
                dagru_forecast, tft_forecast, nbeats_forecast, deepar_forecast
            )
        )
        
        # Anomaly detection features
        features.update(
            EnsembleMetaRiskModel._extract_anomaly_features(
                autoencoder_anomalies, isolation_forest_anomalies, gat_anomalies
            )
        )
        
        # Invoice risk features
        features.update(
            EnsembleMetaRiskModel._extract_invoice_features(tabnet_invoice_risks)
        )
        
        # Traditional risk components
        if traditional_risk and 'components' in traditional_risk:
            components = traditional_risk['components']
            features['trad_cashflow_risk'] = components.get('cashflow_risk', 50)
            features['trad_anomaly_risk'] = components.get('anomaly_risk', 50)
            features['trad_invoice_risk'] = components.get('invoice_risk', 50)
        
        # Historical context features
        if not historical_data.empty:
            features['hist_avg_balance'] = float(
                historical_data['cumulative_cashflow'].mean()
                if 'cumulative_cashflow' in historical_data.columns
                else 0
            )
            features['hist_balance_std'] = float(
                historical_data['cumulative_cashflow'].std()
                if 'cumulative_cashflow' in historical_data.columns
                else 0
            )
            features['hist_data_points'] = float(len(historical_data))
        
        return features
    
    @staticmethod
    def _extract_forecast_features(
        dagru: Dict,
        tft: Dict,
        nbeats: Dict,
        deepar: Dict
    ) -> Dict[str, float]:
        """Extract features from forecasting models"""
        features = {}
        
        # DA-GRU features
        if dagru and 'predictions' in dagru:
            preds = dagru['predictions']
            if 'balance' in preds and preds['balance']:
                features['dagru_min_balance'] = float(min(preds['balance']))
                features['dagru_final_balance'] = float(preds['balance'][-1])
                features['dagru_balance_trend'] = float(
                    (preds['balance'][-1] - preds['balance'][0]) / len(preds['balance'])
                )
            
            if 'attention_scores' in dagru and 'input_attention' in dagru['attention_scores']:
                features['dagru_attention_focus'] = float(
                    max(dagru['attention_scores']['input_attention'])
                )
        
        # TFT features
        if tft and 'predictions' in tft:
            preds = tft['predictions']
            if 'cashflow' in preds and preds['cashflow']:
                features['tft_cashflow_sum'] = float(sum(preds['cashflow']))
                features['tft_negative_days'] = float(
                    sum(1 for cf in preds['cashflow'] if cf < 0)
                )
        
        # N-BEATS features
        if nbeats and 'predictions' in nbeats:
            preds = nbeats['predictions']
            if 'balance' in preds and preds['balance']:
                features['nbeats_min_balance'] = float(min(preds['balance']))
            
            if 'decomposition' in nbeats:
                decomp = nbeats['decomposition']
                features['nbeats_trend_strength'] = decomp.get('trend_strength', 0)
                features['nbeats_trend_up'] = 1.0 if decomp.get('trend_direction') == 'upward' else 0.0
        
        # DeepAR features
        if deepar and 'predictions' in deepar:
            preds = deepar['predictions']
            if 'mean' in preds and preds['mean']:
                features['deepar_mean_balance'] = float(np.mean(preds['mean']))
            
            if 'uncertainty' in deepar:
                uncertainty = deepar['uncertainty']
                if 'probability_negative' in uncertainty:
                    features['deepar_prob_negative'] = float(
                        max(uncertainty['probability_negative'])
                    )
        
        # Consensus features (agreement between models)
        min_balances = []
        if 'dagru_min_balance' in features:
            min_balances.append(features['dagru_min_balance'])
        if 'nbeats_min_balance' in features:
            min_balances.append(features['nbeats_min_balance'])
        
        if len(min_balances) >= 2:
            features['forecast_consensus'] = float(np.std(min_balances))
        
        return features
    
    @staticmethod
    def _extract_anomaly_features(
        autoencoder: List[Dict],
        isolation_forest: List[Dict],
        gat: List[Dict]
    ) -> Dict[str, float]:
        """Extract features from anomaly detection models"""
        features = {}
        
        # Autoencoder anomalies
        if autoencoder:
            features['ae_anomaly_count'] = float(len(autoencoder))
            features['ae_max_severity'] = float(
                max([a.get('severity', 0) for a in autoencoder])
            )
            features['ae_avg_severity'] = float(
                np.mean([a.get('severity', 0) for a in autoencoder])
            )
        else:
            features['ae_anomaly_count'] = 0.0
            features['ae_max_severity'] = 0.0
            features['ae_avg_severity'] = 0.0
        
        # Isolation Forest anomalies
        if isolation_forest:
            features['if_anomaly_count'] = float(len(isolation_forest))
            features['if_max_severity'] = float(
                max([a.get('severity', 0) for a in isolation_forest])
            )
        else:
            features['if_anomaly_count'] = 0.0
            features['if_max_severity'] = 0.0
        
        # GAT anomalies
        if gat:
            features['gat_anomaly_count'] = float(len(gat))
            features['gat_max_severity'] = float(
                max([a.get('severity', 0) for a in gat])
            )
            
            # Count relational vs pattern anomalies
            relational = sum(1 for a in gat if 'relational' in a.get('type', ''))
            features['gat_relational_ratio'] = float(relational / len(gat)) if gat else 0.0
        else:
            features['gat_anomaly_count'] = 0.0
            features['gat_max_severity'] = 0.0
            features['gat_relational_ratio'] = 0.0
        
        # Cross-model anomaly consensus
        all_anomalies = autoencoder + isolation_forest + gat
        if all_anomalies:
            features['total_anomaly_count'] = float(len(all_anomalies))
            features['anomaly_severity_std'] = float(
                np.std([a.get('severity', 0) for a in all_anomalies])
            )
        
        return features
    
    @staticmethod
    def _extract_invoice_features(tabnet_risks: List[Dict]) -> Dict[str, float]:
        """Extract features from TabNet invoice predictions"""
        features = {}
        
        if tabnet_risks:
            risk_scores = [inv.get('predicted_risk_score', 0) for inv in tabnet_risks]
            amounts = [inv.get('amount', 0) for inv in tabnet_risks]
            
            features['invoice_count'] = float(len(tabnet_risks))
            features['invoice_avg_risk'] = float(np.mean(risk_scores))
            features['invoice_max_risk'] = float(max(risk_scores))
            features['invoice_total_amount'] = float(sum(amounts))
            
            # High-risk invoices
            high_risk = sum(1 for score in risk_scores if score > 70)
            features['invoice_high_risk_count'] = float(high_risk)
            features['invoice_high_risk_ratio'] = float(high_risk / len(tabnet_risks))
        else:
            features['invoice_count'] = 0.0
            features['invoice_avg_risk'] = 0.0
            features['invoice_max_risk'] = 0.0
            features['invoice_total_amount'] = 0.0
            features['invoice_high_risk_count'] = 0.0
            features['invoice_high_risk_ratio'] = 0.0
        
        return features
    
    @staticmethod
    def _gradient_boosting_ensemble(
        features: Dict[str, float]
    ) -> Tuple[float, float, Dict[str, float]]:
        """
        Apply gradient boosting to combine features into E-Risk Score
        """
        # Convert features to array
        feature_names = sorted(features.keys())
        feature_values = np.array([features[k] for k in feature_names]).reshape(1, -1)
        
        # Feature weights (learned importance - simulated here)
        # In production, this would be a trained GradientBoostingRegressor
        weights = EnsembleMetaRiskModel._get_feature_weights(feature_names)
        
        # Normalize features
        scaler = StandardScaler()
        
        # Use historical mean/std for normalization (simulated)
        feature_values_norm = np.clip(feature_values / 100, -3, 3)
        
        # Calculate weighted score
        weighted_features = feature_values_norm[0] * weights
        
        # Ensemble score with gradient boosting simulation
        base_score = np.sum(weighted_features)
        
        # Apply non-linear transformations (gradient boosting trees simulation)
        boosted_score = EnsembleMetaRiskModel._simulate_gradient_boosting(
            feature_values_norm[0], weights
        )
        
        # Scale to 0-100
        e_risk_score = 50 + boosted_score * 50  # Center at 50, range Â±50
        e_risk_score = max(0, min(100, e_risk_score))
        
        # Calculate confidence based on model agreement
        confidence = EnsembleMetaRiskModel._calculate_confidence(features)
        
        # Calculate individual model contributions
        model_contributions = EnsembleMetaRiskModel._calculate_model_contributions(
            features, weights, feature_names
        )
        
        return float(e_risk_score), float(confidence), model_contributions
    
    @staticmethod
    def _get_feature_weights(feature_names: List[str]) -> np.ndarray:
        """Get feature importance weights"""
        weights = np.ones(len(feature_names))
        
        # Assign higher weights to key features
        for i, name in enumerate(feature_names):
            if 'min_balance' in name or 'prob_negative' in name:
                weights[i] = 2.0  # Forecast-based features
            elif 'severity' in name or 'anomaly_count' in name:
                weights[i] = 1.5  # Anomaly features
            elif 'invoice' in name and 'risk' in name:
                weights[i] = 1.8  # Invoice risk features
            elif 'trad_' in name:
                weights[i] = 1.2  # Traditional risk features
        
        # Normalize weights
        weights = weights / np.sum(weights)
        
        return weights
    
    @staticmethod
    def _simulate_gradient_boosting(
        features: np.ndarray,
        weights: np.ndarray,
        n_estimators: int = 10
    ) -> float:
        """Simulate gradient boosting ensemble"""
        score = 0.0
        
        # Simulate multiple weak learners (decision stumps)
        for i in range(n_estimators):
            # Each estimator focuses on different feature subsets
            subset_size = max(1, len(features) // 3)
            subset_indices = np.random.choice(len(features), subset_size, replace=False)
            
            # Weak learner prediction
            subset_features = features[subset_indices]
            subset_weights = weights[subset_indices]
            
            # Simple decision stump
            weighted_sum = np.sum(subset_features * subset_weights)
            stump_pred = np.tanh(weighted_sum)  # Non-linear activation
            
            # Add with learning rate
            learning_rate = 0.1
            score += learning_rate * stump_pred
        
        return score
    
    @staticmethod
    def _calculate_confidence(features: Dict[str, float]) -> float:
        """Calculate confidence in the ensemble prediction"""
        confidence_factors = []
        
        # Factor 1: Data availability
        data_completeness = min(1.0, features.get('hist_data_points', 0) / 90)
        confidence_factors.append(data_completeness)
        
        # Factor 2: Model consensus (low variance = high consensus)
        if 'forecast_consensus' in features:
            consensus = 1.0 / (1.0 + features['forecast_consensus'] / 10000)
            confidence_factors.append(consensus)
        
        # Factor 3: Anomaly clarity
        if 'anomaly_severity_std' in features:
            anomaly_clarity = 1.0 - min(1.0, features['anomaly_severity_std'])
            confidence_factors.append(anomaly_clarity)
        else:
            confidence_factors.append(0.8)
        
        # Factor 4: Historical stability
        if 'hist_balance_std' in features and features.get('hist_avg_balance', 0) > 0:
            stability = 1.0 / (1.0 + features['hist_balance_std'] / features['hist_avg_balance'])
            confidence_factors.append(stability)
        
        # Overall confidence
        overall_confidence = np.mean(confidence_factors)
        
        return overall_confidence
    
    @staticmethod
    def _calculate_model_contributions(
        features: Dict[str, float],
        weights: np.ndarray,
        feature_names: List[str]
    ) -> Dict[str, float]:
        """Calculate contribution of each model category"""
        contributions = {
            'forecasting_models': 0.0,
            'anomaly_detection': 0.0,
            'invoice_risk': 0.0,
            'traditional_risk': 0.0,
            'historical_context': 0.0
        }
        
        for i, name in enumerate(feature_names):
            # Use weight only (not weighted value) to avoid scale bias
            weight = weights[i]
            
            if any(model in name for model in ['dagru', 'tft', 'nbeats', 'deepar', 'forecast_consensus']):
                contributions['forecasting_models'] += weight
            elif any(model in name for model in ['ae_', 'if_', 'gat_', 'total_anomaly']):
                contributions['anomaly_detection'] += weight
            elif 'invoice' in name:
                contributions['invoice_risk'] += weight
            elif 'trad_' in name:
                contributions['traditional_risk'] += weight
            elif 'hist_' in name:
                contributions['historical_context'] += weight
        
        # Normalize to percentages
        total = sum(abs(v) for v in contributions.values())
        if total > 0:
            contributions = {k: abs(v) / total for k, v in contributions.items()}
        else:
            # If no contributions, distribute equally
            contributions = {k: 0.2 for k in contributions.keys()}
        
        logger.info(f"Model contributions (by weight): {contributions}")
        
        return contributions
    
    @staticmethod
    def _generate_comprehensive_assessment(
        e_risk_score: float,
        confidence: float,
        model_contributions: Dict[str, float],
        features: Dict[str, float],
        all_forecasts: Dict,
        all_anomalies: Dict,
        invoice_risks: List[Dict]
    ) -> Dict[str, any]:
        """Generate comprehensive risk assessment report"""
        # Determine risk level
        if e_risk_score < 30:
            risk_level = "LOW"
            risk_color = "green"
            risk_message = "Financial health is strong with minimal distress indicators"
        elif e_risk_score < 50:
            risk_level = "MEDIUM-LOW"
            risk_color = "lime"
            risk_message = "Stable financial position with some minor concerns"
        elif e_risk_score < 70:
            risk_level = "MEDIUM-HIGH"
            risk_color = "yellow"
            risk_message = "Moderate financial stress detected - monitoring recommended"
        elif e_risk_score < 85:
            risk_level = "HIGH"
            risk_color = "orange"
            risk_message = "Significant financial distress indicators - action required"
        else:
            risk_level = "CRITICAL"
            risk_color = "red"
            risk_message = "Critical financial distress - immediate intervention needed"
        
        # Generate key insights
        key_insights = EnsembleMetaRiskModel._generate_key_insights(
            features, all_forecasts, all_anomalies, invoice_risks
        )
        
        # Generate recommendations
        recommendations = EnsembleMetaRiskModel._generate_ensemble_recommendations(
            e_risk_score, features, model_contributions
        )
        
        return {
            'e_risk_score': round(e_risk_score, 2),
            'risk_level': risk_level,
            'risk_color': risk_color,
            'confidence': round(confidence * 100, 1),
            'confidence_level': 'High' if confidence > 0.8 else 'Medium' if confidence > 0.6 else 'Low',
            'message': risk_message,
            'model_contributions': {
                k: round(v * 100, 1) for k, v in model_contributions.items()
            },
            'key_insights': key_insights,
            'recommendations': recommendations,
            'advanced_metrics': {
                'ensemble_stability': round(confidence, 3),
                'forecast_consensus': features.get('forecast_consensus', 0),
                'total_anomalies_detected': int(features.get('total_anomaly_count', 0)),
                'high_risk_invoices': int(features.get('invoice_high_risk_count', 0))
            }
        }
    
    @staticmethod
    def _generate_key_insights(
        features: Dict[str, float],
        all_forecasts: Dict,
        all_anomalies: Dict,
        invoice_risks: List[Dict]
    ) -> List[str]:
        """Generate key insights from ensemble analysis"""
        insights = []
        
        # Cashflow insights
        if features.get('deepar_prob_negative', 0) > 0.3:
            insights.append(
                f"âš ï¸ {features['deepar_prob_negative']*100:.0f}% probability of negative balance in forecast period"
            )
        
        # Trend insights
        if features.get('nbeats_trend_up', 0) == 0:
            insights.append("ðŸ“‰ Downward cashflow trend detected by N-BEATS decomposition")
        
        # Anomaly insights
        total_anomalies = features.get('total_anomaly_count', 0)
        if total_anomalies > 5:
            insights.append(f"ðŸ” {int(total_anomalies)} financial anomalies detected across multiple models")
        
        # Invoice insights
        if features.get('invoice_high_risk_ratio', 0) > 0.3:
            insights.append(
                f"ðŸ“‹ {features['invoice_high_risk_ratio']*100:.0f}% of invoices flagged as high-risk by TabNet"
            )
        
        # Attention insights
        if features.get('dagru_attention_focus', 0) > 0.5:
            insights.append("ðŸŽ¯ DA-GRU attention mechanism identifies critical cashflow patterns")
        
        # Add at least one positive insight if available
        if features.get('hist_avg_balance', 0) > 0 and len(insights) < 3:
            insights.append(
                f"âœ… Historical average balance maintained at â‚¹{features['hist_avg_balance']:,.0f}"
            )
        
        return insights[:5]  # Top 5 insights
    
    @staticmethod
    def _generate_ensemble_recommendations(
        e_risk_score: float,
        features: Dict[str, float],
        model_contributions: Dict[str, float]
    ) -> List[Dict[str, str]]:
        """Generate actionable recommendations based on ensemble analysis"""
        recommendations = []
        
        # High-level strategic recommendations
        if e_risk_score > 70:
            recommendations.append({
                'priority': 'Critical',
                'action': 'Immediate Cashflow Intervention',
                'description': 'Deploy emergency cashflow measures - accelerate collections, defer payments',
                'impact': 'High'
            })
        
        # Model-specific recommendations
        if model_contributions.get('forecasting_models', 0) > 0.3:
            if features.get('dagru_min_balance', 0) < 0:
                recommendations.append({
                    'priority': 'High',
                    'action': 'Address Forecasted Liquidity Gap',
                    'description': 'DA-GRU predicts negative balance - secure credit line or delay expenses',
                    'impact': 'High'
                })
        
        if model_contributions.get('anomaly_detection', 0) > 0.3:
            if features.get('ae_max_severity', 0) > 0.7:
                recommendations.append({
                    'priority': 'High',
                    'action': 'Investigate Expense Anomalies',
                    'description': 'Deep autoencoder detected unusual spending patterns - audit required',
                    'impact': 'Medium'
                })
        
        if model_contributions.get('invoice_risk', 0) > 0.25:
            if features.get('invoice_high_risk_count', 0) > 2:
                recommendations.append({
                    'priority': 'Medium',
                    'action': 'Prioritize Invoice Collections',
                    'description': f"{int(features['invoice_high_risk_count'])} invoices at high delay risk - follow up immediately",
                    'impact': 'Medium'
                })
        
        # General recommendations
        if e_risk_score < 50:
            recommendations.append({
                'priority': 'Low',
                'action': 'Maintain Current Strategy',
                'description': 'Financial health is stable - continue monitoring',
                'impact': 'Low'
            })
        
        return recommendations
